# -*- coding: utf-8 -*-
"""Deforestation Detection (Fire Classification) Completed Project

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1OKrSL_yJ3CHJIKITeT5WeHFZair2LitS

# **Classification of Fire Types in India Using MODIS Satellite Data (2021–2023)**

India witnesses various types of fire incidents annually, including forest fires, agricultural burning, volcanic activity, and other thermal anomalies. Accurate identification of fire sources is crucial for timely disaster response, environmental monitoring, and resource management. The MODIS sensors aboard NASA’s Terra and Aqua satellites provide reliable, near real-time thermal anomaly data globally, including for India.

While the MODIS dataset includes rich geospatial and thermal parameters, the challenge lies in correctly classifying the type of fire event — whether it stems from vegetation, volcanoes, static land sources, or offshore sources — using satellite-captured features.

**Dataset** : **MODIS Dataset**

**Importing the libraries**

pip install numpy pandas matplotlib seaborn scikit-learn folium
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import  accuracy_score,classification_report,ConfusionMatrixDisplay
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder, OneHotEncoder

"""**Loading the dataset**"""

df1=pd.read_csv('/content/modis_2021_India.csv')
df2=pd.read_csv('/content/modis_2022_India.csv')
df3=pd.read_csv('/content/modis_2023_India.csv')

"""**Printing the first 5 rows**"""

df1.head()

df2.head()

df3.head()

df=pd.concat([df1,df2,df3],ignore_index=True)
df.head()

df.shape

df.info()

df.isnull().sum()

df.duplicated().sum()

df.columns

df.describe().T

df.type.value_counts()

"""**Exploratory Data Analysis**"""

# Checking unique and n unique values for all categorical features
for col in df.columns:
  if df[col].dtype == 'object':
    print(f"Column: {col}")
    print(f"Unique values: {df[col].unique()}")
    print(f"Number of unique values: {df[col].nunique()}")
    print("-" * 50)

# Count plot for 'type'
plt.figure(figsize=(8,6))
sns.countplot(x='type', data=df)
plt.title('Distribution of Fire Types')
plt.xlabel('Fire Type')
plt.ylabel('Count')
plt.show()

"""

*  The count plot shows the distribution of different fire types in the dataset.
*  It is evident that 'MODIS' is the most frequent fire type, followed by 'VIIRS'.
*  The 'type' variable appears to be unbalanced, with 'MODIS' having significantly more observations than 'VIIRS'. This imbalance might need to be considered during model training.



"""

# Histogram of 'confidence'
plt.figure(figsize=(8, 6))
sns.histplot(df['confidence'], bins=20, kde=True)
plt.title('Distribution of Confidence')
plt.xlabel('Confidence')
plt.ylabel('Frequency')
plt.show()

"""*   The histogram illustrates the distribution of the 'confidence' feature.
*   The distribution appears to be bimodal, with peaks around low confidence values and high confidence values.
*   There are fewer observations in the middle range of confidence.
*  This suggests that observations are often recorded with either low confidence or high confidence.




"""

# Box plot for 'confidence' by 'type'
plt.figure(figsize=(8, 6))
sns.boxplot(x='type', y='confidence', data=df)
plt.title('Confidence by Fire Type')
plt.xlabel('Fire Type')
plt.ylabel('Confidence')
plt.show()

"""

*   The box plot shows the distribution of 'confidence' for each fire type
*   Both 0 and 2 have a wide range of confidence values.
*   The median confidence for both types appears to be in the higher range.
*   There are some outliers, particularly for the 'MODIS' type, indicating observations with unusually low or high confidence.



"""

# Scatter plot of 'latitude' vs 'longitude'
plt.figure(figsize=(10, 8))
sns.scatterplot(x='longitude', y='latitude', data=df, hue='type', s=10)
plt.title('Fire Locations by Type')
plt.xlabel('Longitude')
plt.ylabel('Latitude')
plt.legend(title='Fire Type')
plt.show()

"""

*   The scatter plot visualizes the geographical distribution of fire locations, colored by fire type.
*   It provides a visual representation of where fires are occurring based on latitude and longitude.
*   Different fire types might be concentrated in specific geographical areas, which could be a useful feature for modeling.
*   The density of points indicates areas with higher fire activity.








"""

# Count plot for 'daynight'
plt.figure(figsize=(6, 4))
sns.countplot(x='daynight', data=df)
plt.title('Distribution of Day/Night Observations')
plt.xlabel('Day/Night')
plt.ylabel('Count')
plt.show()

"""

*   The count plot for 'daynight' shows whether the fire observations were made during the day or night.
*   It indicates the proportion of day versus night observations in the dataset.
*  Knowing the distribution of day/night observations can be relevant as detection capabilities or fire behavior might differ between day and night.




"""

# Count plot for 'Satellite'
plt.figure(figsize=(6, 4))
sns.countplot(x='satellite', data=df)
plt.title('Distribution of Satellite Observations')
plt.xlabel('Satellite')
plt.ylabel('Count')
plt.show()

"""

*  This count plot shows the distribution of observations made by different satellites.
*   It reveals which satellites contributed the most data to the dataset.
*   Understanding the satellite distribution can be important as different satellites may have different characteristics or coverage.



"""

# Count plot for 'version'
plt.figure(figsize=(6, 4))
sns.countplot(x='version', data=df)
plt.title('Distribution of Version')
plt.xlabel('Version')
plt.ylabel('Count')
plt.show()

#this code take more time
#Pairplot for numerical features (subset)
#sns.pairplot(df[['latitude', 'longitude', 'brightness', 'confidence', 'frp', 'type']], hue='type', diag_kind='kde')
#plt.suptitle('Pairplot of Numerical Features')
#plt.show()

"""The pairplot provides a matrix of scatter plots for all pairs of numerical features and histograms/KDE plots on the diagonal for each feature, separated by the 'type' variable. Here are some insights from the pairplot:



*  **Individual Feature Distributions (Diagonal):** The diagonal plots (histograms/KDEs) show the distribution of each numerical feature for each fire type.

1. latitude and longitude: These show the
geographical distribution, reinforcing the scatter plot observation. Different fire types appear to be concentrated in certain geographical areas.
2. brightness: The distribution of brightness values can be compared between fire types. There might be differences in the typical brightness of fires detected by MODIS versus VIIRS.
3. confidence: This shows the distribution of confidence for each type, similar to the earlier box plot but as a histogram/KDE. It can highlight differences in the confidence levels associated with each fire type.
4. frp: The distribution of fire radiative power (FRP) can be compared. This might reveal if one fire type tends to have significantly higher or lower FRP values than the other.


*   **Relationships Between Features (Off-Diagonal Scatter Plots):** The off-diagonal scatter plots show the relationship between pairs of numerical features, colored by fire type.

1. latitude vs. longitude: As seen before, this visualizes the geographical distribution by type.
2. brightness vs. confidence: This plot shows the relationship between brightness and confidence. Is there a correlation? Does higher brightness tend to correlate with higher confidence? How does this relationship differ between fire types?
3. brightness vs. frp: This shows the relationship between brightness and fire radiative power. These two features are likely related. The plot can reveal the strength and nature of this relationship and whether it varies by fire type.
4. confidence vs. frp: This visualizes the relationship between confidence and FRP. Does higher FRP tend to result in higher confidence? How does this relationship differ for different fire types?
5. Other pairs: Examine the relationships between latitude/longitude and the other numerical features (brightness, confidence, frp). Are there geographical patterns in these features?




"""

# Heatmap of correlations between numerical features
plt.figure(figsize=(10, 8))
correlation_matrix = df[['latitude', 'longitude', 'brightness', 'confidence', 'frp']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

"""The heatmap visualizes the Pearson correlation coefficients between the numerical features: 'latitude', 'longitude', 'brightness', 'confidence', and 'frp'. The values range from -1 to 1, where:

* 1 indicates a perfect positive linear correlation.
* -1 indicates a perfect negative linear correlation.
* 0 indicates no linear correlation.
* The color intensity and the annotation (annot=True) help in quickly identifying the strength and direction of the relationships.

**Key insights from the heatmap:**

1. **High Correlation between brightness and frp:** There appears to be a strong positive correlation between 'brightness' and 'frp'. This is expected as both features are related to the intensity of the fire. Higher brightness is likely to be associated with higher fire radiative power. This strong correlation might indicate multicollinearity if both features are used directly in a linear model, but can also be insightful for understanding the data.

2. **Moderate Correlation between brightness and confidence:** There seems to be a moderate positive correlation between 'brightness' and 'confidence'. This suggests that brighter fire detections tend to be associated with higher confidence levels.

3. **Moderate Correlation between frp and confidence:** Similarly, there is likely a moderate positive correlation between 'frp' and 'confidence'. Fires with higher radiative power might be easier to detect and thus have higher confidence scores.

4. **Low Correlation with Geographical Features:** The correlations between 'latitude' and 'longitude' with 'brightness', 'confidence', and 'frp' appear to be relatively low. This suggests that the intensity or confidence of a fire detection is not strongly linearly related to its geographical location. While there might be spatial patterns as seen in the scatter plot, a simple linear correlation doesn't capture them strongly.

5. **Correlation between latitude and longitude:** The correlation between 'latitude' and 'longitude' is often low unless there's a specific geographical pattern in the data that aligns linearly. In this case, it's likely low, indicating that fires are distributed across various locations without a strong linear relationship between their latitude and longitude coordinates within the dataset.

Overall, the heatmap provides a concise overview of the linear relationships between the numerical features. It highlights the expected strong correlations between features related to fire intensity (brightness, frp, confidence) and shows that geographical coordinates have weaker linear relationships with these intensity measures. This information can be valuable for feature selection, understanding feature interactions, and guiding the choice of modeling techniques.
"""

numerical_cols = df.select_dtypes(include=np.number).columns

numerical_cols

numerical_cols = ['brightness', 'scan', 'track', 'acq_time','confidence', 'version', 'bright_t31', 'frp']
df[numerical_cols].hist(bins=50, figsize=(15, 10))
plt.suptitle('Histograms of Numerical Features')
plt.show()

"""* 'brightness': The distribution of brightness values. This shows the range of detected fire brightness and where the values tend to cluster. It might reveal if fires tend to be of low, medium, or high brightness.
* 'scan': The distribution of scan sizes. This feature relates to the size of the pixel footprint. The histogram shows the typical scan sizes in the dataset.
* 'track': Similar to scan, this relates to the track size. The histogram shows the distribution of track sizes.
* 'acq_time': The distribution of acquisition times (likely represented as a numerical value like time of day). This histogram can reveal patterns in when fires are detected (e.g., more detections during certain hours).
* 'confidence': The distribution of confidence scores. This is a numerical representation of the earlier confidence histogram and box plot. It reinforces the bimodal nature observed earlier.
* 'version': The distribution of different version values. This shows the frequency of observations from different processing versions.
* 'bright_t31': The distribution of brightness temperature at band 31. This is another measure related to fire intensity. Its distribution can be compared to 'brightness'.
* 'frp': The distribution of fire radiative power. This shows the typical FRP values in the dataset and their range. It complements the 'brightness' histogram in understanding fire intensity.
* 'type': While 'type' is included in the numerical columns list due to its representation, its histogram will show the distribution of the encoded numerical values for fire types. This visually confirms the class imbalance seen in the count plot.
-Overall, these histograms provide a detailed look at the individual distributions of the numerical features. They help in understanding the range, central tendency, and variability of each feature, identifying potential outliers, and assessing the shape of the distribution (e.g., normal, skewed, bimodal). This information is crucial for data preprocessing, feature understanding, and selecting appropriate modeling techniques.
"""

import statsmodels.api as sm
import scipy.stats as stats

# List of numerical features to check for distribution
numerical_features = ['brightness', 'confidence', 'frp', 'bright_t31', 'scan', 'track']

for feature in numerical_features:
    print(f"Analyzing distribution for: {feature}")

    # KDE Plot
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    sns.kdeplot(df[feature], fill=True)
    plt.title(f'KDE Plot of {feature}')
    plt.xlabel(feature)
    plt.ylabel('Density')

    # QQ Plot
    plt.subplot(1, 2, 2)
    stats.probplot(df[feature], dist="norm", plot=plt)
    plt.title(f'QQ Plot of {feature}')

    plt.tight_layout()
    plt.show()
    print("-" * 50)

"""'brightness': Distribution is skewed and bimodal, QQ plot shows significant deviation from normality.

'confidence': Distribution is bimodal with peaks at low and high values, QQ plot confirms non-normality, especially in the tails.

'frp': Distribution is highly skewed to the right, QQ plot shows a strong departure from the normal distribution, particularly for larger values.

'bright_t31': Distribution appears somewhat skewed, QQ plot indicates deviation from normality, especially at the extremes.

'scan': Distribution is concentrated at lower values with a tail towards higher values, QQ plot suggests non-normality.

'track': Distribution is concentrated at lower values with a tail towards higher values, QQ plot suggests non-normality.
"""

# --- Temporal Analysis ---
# Convert 'acq_date' to datetime objects
df['acq_date'] = pd.to_datetime(df['acq_date'])
# Extract temporal features
df['year'] = df['acq_date'].dt.year
df['month'] = df['acq_date'].dt.month
df['day_of_week'] = df['acq_date'].dt.dayofweek # Monday=0, Sunday=6
df['day_of_year'] = df['acq_date'].dt.dayofyear
# Assuming acq_time is in HHMM format, convert to integer after handling NaNs
df['hour'] = df['acq_time'].fillna(0).astype(int) // 100 # Fill NaN with 0 and convert to integer, then extract hour

"""1. Extracting Temporal Features: It converts the acq_date column to datetime objects and extracts new features like year, month, day_of_week, day_of_year, and hour from the acquisition date and time.
2. Visualizing Temporal Distributions: It generates count plots to show:
     * The number of fire detections per month.
     * The number of fire detections per day of the week.
"""

# Visualize fire detections over months
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='month', palette='viridis')
plt.title('Fire Detections by Month (2023)')
plt.xlabel('Month')
plt.ylabel('Number of Detections')
plt.xticks(ticks=range(12), labels=['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec'])
plt.show()

# Visualize fire detections by day of the week
plt.figure(figsize=(10, 6))
sns.countplot(data=df, x='day_of_week', palette='viridis')
plt.title('Fire Detections by Day of Week (2023)')
plt.xlabel('Day of Week')
plt.ylabel('Number of Detections')
plt.xticks(ticks=range(7), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
plt.show()

"""# **Outliers and Outlier Treatments**

Outliers: Outliers are data points that are significantly different from other observations in a dataset. They can occur due to measurement errors, data entry mistakes, or genuinely rare events. Outliers can skew statistical analyses (like mean, standard deviation) and impact the performance of machine learning models.
"""

# Visualize outliers using box plots for key numerical features
plt.figure(figsize=(12, 8))
sns.boxplot(data=df[numerical_cols])
plt.title('Box Plots for Key Numerical Features')
plt.ylabel('Value')
plt.show()

"""* 'brightness', 'bright_t31', 'frp': These fire intensity-related features show a wide range and numerous high-value outliers, suggesting that while most fires might have moderate intensity, there are instances of very bright or high-FRP fires. The lower whiskers might also show some outliers on the lower end.

    * 'scan', 'track': These features related to pixel size also show outliers, indicating observations where the scan/track size was significantly different from the typical values.
    * 'confidence': The box plot for confidence, similar to the histogram, likely reinforces the concentration of data at the ends (low and high confidence), with some outliers in the middle range or beyond.
    * 'acq_time': Depending on how 'acq_time' is represented numerically, the box plot could show if there are acquisition times that are significantly different from the usual patterns.
    * 'version', 'type': These are likely represented numerically but are essentially categorical or ordinal. Their box plots might not be as informative as count plots for distribution, but they can still show the spread of other numerical features within each version/type category if plotted against them.
"""

def remove_outliers_iqr(df, column):
  Q1 = df[column].quantile(0.25)
  Q3 = df[column].quantile(0.75)
  IQR = Q3 - Q1
  lower_bound = Q1 - 1.5 * IQR
  upper_bound = Q3 + 1.5 * IQR
  df_cleaned = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)].copy()
  return df_cleaned

# Apply outlier removal to numerical columns
for col in numerical_cols:
  df = remove_outliers_iqr(df, col)

print("Shape after removing outliers:", df.shape)

# Visualize box plots after outlier removal
plt.figure(figsize=(12, 8))
sns.boxplot(data=df[numerical_cols])
plt.title('Box Plots for Numerical Features After Outlier Removal')
plt.ylabel('Value')
plt.show()

"""**Box Plots (After):**

* The individual outlier points above and below the whiskers in the previous box plots have been significantly reduced or eliminated for the treated columns ('brightness', 'scan', 'track', 'bright_t31', 'frp').
* The maximum and minimum values represented by the upper and lower whiskers will be much closer to the bulk of the data, as extreme values have been removed.
* The scale of the y-axis in the box plots for the treated features is likely smaller, as it now focuses on the data within the calculated IQR range.
* The boxes (IQR) and whiskers now represent the distribution of the majority of the cleaned data. While the IQR method removes values outside 1.5IQR from the quartiles, some data points beyond the whiskers might still be present, but they represent the less extreme values within the filtered dataset. The visual spread of the central 50% (the box) and the range covered by the whiskers (typically 1.5IQR) will be more representative of the data after removing the most extreme values.
* For 'confidence', 'acq_time', 'version', and 'type', where outlier removal wasn't explicitly applied in the code snippet, their box plots would show similar distributions as before, potentially still displaying outliers if present in the original data.
"""

df.head()

df.type.value_counts()

categorical_cols = df.select_dtypes(include='object').columns

categorical_cols

# Select categorical columns for encoding
categorical_cols_to_encode = ['daynight', 'satellite', 'instrument']

# Apply One-Hot Encoding
df_encoded = pd.get_dummies(df, columns=categorical_cols_to_encode, drop_first=True)

df_encoded.head(100)

df_encoded.type.value_counts()

"""# pip install folium"""

# !pip install folium
import folium

# Create map and sample data
india_map = folium.Map(location=[22.351115, 78.667743], zoom_start=5)
sample_df = df_encoded.sample(n=min(10000, len(df_encoded)), random_state=42)

# Add markers
for _, row in sample_df.iterrows():
    folium.CircleMarker(
        location=[row['latitude'], row['longitude']],
        radius=3,
        color='red',
        fill=True,
        fill_opacity=0.6,
        popup=f"FRP: {row['frp']:.2f}, Date: {row['acq_date'].strftime('%Y-%m-%d')}"
    ).add_to(india_map)

display(india_map)

"""Folium map of India with red circle markers indicating fire locations based on the 'latitude' and 'longitude' columns of the dataset. Each marker represents a detected fire incident.

Here are some potential insights derived from this map:

1. Geographical Distribution of Fires: The map visually shows where fires are occurring across India. Areas with a higher concentration of markers indicate regions with more frequent fire activity in the dataset's timeframe.
2. Spatial Clusters: You can observe if fires are clustered in specific states, regions, or ecological zones. For example, are fires more common in forested areas, agricultural regions, or industrial zones?
3. Potential Hotspots: Densely packed clusters of markers highlight potential fire hotspots. These are areas that might be particularly vulnerable to fires and could require targeted fire prevention and management efforts.
4. Variability Across India: The map illustrates the spatial variability of fire incidents across the country. Some areas might have very few detections, while others show significant activity.
5. Interaction with Popups: Hovering over or clicking on the markers reveals additional information provided in the popup, specifically the Fire Radiative Power (FRP) and the acquisition date of the fire detection. This allows for a closer inspection of individual fire events, providing context about their intensity and timing.
6. Effectiveness of Sampling: Since the code uses a sample of the data (sample_df), the map shows the distribution based on that sample. If the original dataset is very large, sampling is necessary for visualization, but it's important to remember that the map represents the sample's distribution, not necessarily the entire dataset's distribution perfectly. The effectiveness of the sampling can be assessed visually if the sample density appears to reflect the expected distribution of fires.
7. Temporal Patterns (Limited): While not directly a temporal visualization, the date in the popup allows for some manual exploration of fire events at specific locations over time. However, a dedicated temporal visualization (like an animation or time series analysis) would be more effective for observing how fire locations change over the acquisition period.

In summary, the Folium map provides a powerful visual overview of the spatial distribution of fire incidents across India, helping to identify fire-prone regions and providing a geographical context for the fire data.

**Normalize continuous variables**
"""

scaler = StandardScaler()
numerical_cols_to_scale = ['brightness', 'scan', 'track', 'confidence', 'bright_t31', 'frp']
df_encoded[numerical_cols_to_scale] = scaler.fit_transform(df_encoded[numerical_cols_to_scale])
df_encoded.head()

df_encoded.info()

# Heatmap of correlations between numerical features
plt.figure(figsize=(10, 8))
correlation_matrix = df_encoded[['brightness', 'scan', 'track', 'confidence', 'bright_t31', 'frp']].corr()
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=".2f")
plt.title('Correlation Heatmap of Numerical Features')
plt.show()

df_encoded.head()

df_encoded.type.value_counts()

# Separate features (X) and target variable (y)
# Assuming 'type' is the target variable you want to predict
# Drop temporal features if not intended for prediction task that uses 'type' as target
features = ['brightness', 'scan', 'track', 'confidence', 'bright_t31', 'frp']
target = 'type'

X = df_encoded[features]
y = df_encoded[target]

X

y

!pip install -U imbalanced-learn
from imblearn.over_sampling import SMOTE

# Initialize SMOTE
smote = SMOTE(random_state=42)

# Apply SMOTE to the training data
X_resampled, y_resampled = smote.fit_resample(X, y)

# Check the distribution of the target variable after resampling
print("Distribution of target variable after SMOTE:")
print(y_resampled.value_counts())

# Split the dataset into training and testing sets
# You can adjust the test_size and random_state as needed
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.25, random_state=42, stratify=y_resampled)

print("Shape of X_train:", X_train.shape)
print("Shape of X_test:", X_test.shape)
print("Shape of y_train:", y_train.shape)
print("Shape of y_test:", y_test.shape)

# Import Logistic Regression to Train from SKlearn
loreg = LogisticRegression(max_iter=200)
loreg.fit(X_train,y_train)
loreg_pred = loreg.predict(X_test)
score = accuracy_score(y_test,loreg_pred)
cr = classification_report(y_test,loreg_pred)

print("Logistic Regression")
print ("Accuracy Score value: {:.4f}".format(score))
print (cr)

from sklearn.tree import DecisionTreeClassifier
dtc = DecisionTreeClassifier()
dtc.fit(X_train,y_train)
dtc_pred = dtc.predict(X_test)
score = accuracy_score(y_test,dtc_pred)
cr = classification_report(y_test,dtc_pred)

print("Decision Tree")
print ("Accuracy Score value: {:.4f}".format(score))
print (cr)

dt_cm = ConfusionMatrixDisplay.from_estimator(dtc, X_test, y_test)

rfc = RandomForestClassifier()
rfc.fit(X_train,y_train)
rfc_pred = rfc.predict(X_test)
score = accuracy_score(y_test,rfc_pred)
cr = classification_report(y_test,rfc_pred)

print("Random Forest")
print ("Accuracy Score value: {:.4f}".format(score))
print (cr)

rf_cm = ConfusionMatrixDisplay.from_estimator(rfc, X_test, y_test)

# KNeighborsClassifier to Train from SKlearn
knnc = KNeighborsClassifier()
knnc.fit(X_train,y_train)
knn_pred = knnc.predict(X_test)
score = accuracy_score(y_test,knn_pred)
cr = classification_report(y_test,knn_pred)

print("KNeighbors Classifier")
print ("Accuracy Score value: {:.4f}".format(score))
print (cr)

knn_cm = ConfusionMatrixDisplay.from_estimator(knnc, X_test, y_test)

from xgboost import XGBClassifier
from sklearn.preprocessing import LabelEncoder

# Map the target labels to a contiguous range for XGBoost
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)


xgb = XGBClassifier(use_label_encoder=False, eval_metric='logloss')
xgb.fit(X_train, y_train_encoded)
xgb_pred = xgb.predict(X_test)
score = accuracy_score(y_test_encoded, xgb_pred)
cr = classification_report(y_test_encoded, xgb_pred)

print("XGBoost Classifier")
print("Accuracy Score value: {:.4f}".format(score))
print(cr)

xgb = ConfusionMatrixDisplay.from_estimator(xgb, X_test, y_test)

from sklearn.ensemble import GradientBoostingClassifier

gbc = GradientBoostingClassifier()
gbc.fit(X_train, y_train)
gbc_pred = gbc.predict(X_test)
score = accuracy_score(y_test, gbc_pred)
cr = classification_report(y_test, gbc_pred)

print("Gradient Boosting Classifier")
print("Accuracy Score value: {:.4f}".format(score))
print(cr)

gbc = ConfusionMatrixDisplay.from_estimator(gbc, X_test, y_test)

from lightgbm import LGBMClassifier

lgbm = LGBMClassifier()
lgbm.fit(X_train, y_train)
lgbm_pred = lgbm.predict(X_test)
score = accuracy_score(y_test, lgbm_pred)
cr = classification_report(y_test, lgbm_pred)

print("LightGBM Classifier")
print("Accuracy Score value: {:.4f}".format(score))
print(cr)

lgbm = ConfusionMatrixDisplay.from_estimator(lgbm, X_test, y_test)

from catboost import CatBoostClassifier

catboost = CatBoostClassifier(verbose=0)
catboost.fit(X_train, y_train)
cat_pred = catboost.predict(X_test)
score = accuracy_score(y_test, cat_pred)
cr = classification_report(y_test, cat_pred)

print("CatBoost Classifier")
print("Accuracy Score value: {:.4f}".format(score))
print(cr)

catboost = ConfusionMatrixDisplay.from_estimator(catboost, X_test, y_test)

from sklearn.neural_network import MLPClassifier

mlp = MLPClassifier(max_iter=500)
mlp.fit(X_train, y_train)
mlp_pred = mlp.predict(X_test)
score = accuracy_score(y_test, mlp_pred)
cr = classification_report(y_test, mlp_pred)

print("Multi-Layer Perceptron Classifier")
print("Accuracy Score value: {:.4f}".format(score))
print(cr)

mlp = ConfusionMatrixDisplay.from_estimator(mlp, X_test, y_test)

# Collect the accuracies of each model
model_accuracies = {
    "Logistic Regression": accuracy_score(y_test, loreg_pred),
    "Decision Tree": accuracy_score(y_test, dtc_pred),
    "Random Forest": accuracy_score(y_test, rfc_pred),
    "KNeighbors Classifier": accuracy_score(y_test, knn_pred),
    "XGBoost Classifier": accuracy_score(y_test_encoded, xgb_pred), # Use y_test_encoded for comparison
    "GradientBoostingClassifier": accuracy_score(y_test, gbc_pred),
    "LightGBM Classifier": accuracy_score(y_test, lgbm_pred),
    "CatBoost Classifier": accuracy_score(y_test, cat_pred),
    "Neural Network (MLPClassifier)": accuracy_score(y_test, mlp_pred)
}

# Find the best model
best_model_name = max(model_accuracies, key=model_accuracies.get)
best_model_accuracy = model_accuracies[best_model_name]

print("Model Accuracies:")
for model, accuracy in model_accuracies.items():
    print(f"{model}: {accuracy:.4f}")

print(f"\nBest Model: {best_model_name} with Accuracy: {best_model_accuracy:.4f}")

import joblib

# Save the best model
# Based on the previous output, let's assume Random Forest was the best model.
best_model = rfc

joblib.dump(best_model, 'best_fire_detection_model.pkl')

# Save the StandardScaler instance
joblib.dump(scaler, 'scaler.pkl')

print("Best model and scaler saved successfully.")

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score

# Import classifiers
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier


# Split data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Scale data
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Define models
models = {
    'Decision Tree': DecisionTreeClassifier(),
    'Logistic Regression': LogisticRegression(max_iter=1000),
    'Random Forest': RandomForestClassifier(),
    'KNN': KNeighborsClassifier(),
    'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='mlogloss'), # Removed use_label_encoder as it's deprecated
    'Gradient Boosting': GradientBoostingClassifier(),
    'LightGBM': LGBMClassifier(),
    'CatBoost': CatBoostClassifier(verbose=0),
    'MLP (Neural Net)': MLPClassifier(max_iter=1000)
}

# Train and evaluate models
results = []

# Encode target variable for models that require it
le = LabelEncoder()
y_train_encoded = le.fit_transform(y_train)
y_test_encoded = le.transform(y_test)


for name, model in models.items():
    print(f"Training {name}...")
    if name in ['XGBoost', 'CatBoost', 'LightGBM', 'MLP (Neural Net)']:
        model.fit(X_train_scaled, y_train_encoded)
        y_pred_encoded = model.predict(X_test_scaled)
        y_pred = le.inverse_transform(y_pred_encoded)
    else:
        model.fit(X_train_scaled, y_train)
        y_pred = model.predict(X_test_scaled)


    results.append({
        'Model': name,
        'Accuracy': accuracy_score(y_test, y_pred),
        'Precision': precision_score(y_test, y_pred, average='weighted', zero_division=0),
        'Recall': recall_score(y_test, y_pred, average='weighted'),
        'F1 Score': f1_score(y_test, y_pred, average='weighted')
    })

df_results = pd.DataFrame(results)
print("\nModel Performance:")
display(df_results)

import matplotlib.pyplot as plt
import seaborn as sns
import pandas as pd

# Create a DataFrame based on the image data
data = {
    'Model': [
        'Decision Tree', 'Logistic Regression', 'Random Forest', 'KNN',
        'XGBoost', 'Gradient Boosting', 'LightGBM', 'CatBoost', 'MLP (Neural Net)'
    ],
    'Accuracy': [0.928896, 0.967049, 0.966917, 0.965940, 0.966943, 0.966864, 0.964303, 0.967207, 0.967075],
    'Precision': [0.939190, 0.935183, 0.949414, 0.944832, 0.948429, 0.939297, 0.940691, 0.956797, 0.952850],
    'Recall': [0.928896, 0.967049, 0.966917, 0.965940, 0.966943, 0.966864, 0.964303, 0.967207, 0.967075],
    'F1 Score': [0.933950, 0.950849, 0.951497, 0.951559, 0.951107, 0.950809, 0.950398, 0.951598, 0.951276]
}

df = pd.DataFrame(data)

# Plot style
sns.set(style='whitegrid')
metrics = ['Accuracy', 'Precision', 'Recall', 'F1 Score']
plt.figure(figsize=(14, 10))

# Plotting 4 subplots for each metric
for i, metric in enumerate(metrics):
    plt.subplot(2, 2, i + 1)
    sorted_df = df.sort_values(by=metric, ascending=False)
    sns.barplot(x=metric, y='Model', data=sorted_df, palette='coolwarm')
    plt.title(f'{metric} Comparison', fontsize=13)
    plt.xlabel(metric)
    plt.ylabel("")

plt.tight_layout()
plt.show()

"""# **Model Deployment**"""

pip install streamlit

import streamlit as st
import numpy as np
import joblib
#Load model and scalar
model = joblib.load('best_fire_detection_model.pkl')
scaler = joblib.load('scaler.pkl')
#set page title
st.set_page_config(page_title='Fire Type Classifier', layout='centered')
#set page header
st.title('🔥 Fire Detection App')
st.markdown("Predict fire type based on MODIS satellite readings.")

#User input fields for 6 features
brightness=st.number_input("Brightness", value=300.00)
bright_t31=st.number_input("Bright_t31", value=290.00)
frp=st.number_input("Fire Radiative Power (FRP)", value=15.0)
scan=st.number_input("Scan", value=1.0)
track=st.number_input("Track", value=1.0)
confidence=st.selectbox("Confidence level", ["low","nominal","high"])

#Map confidence to numeric
confidence_map = {"low":0, "nominal":1, "high":2}
confidence_val = confidence_map[confidence]

#Combine and scale input
input_data = np.array([[brightness, scan, track, confidence_val, bright_t31, frp]])
scaled_input = scaler.transform(input_data)

#Predict and display
if st.button("Predict Fire Type"):
  prediction = model.predict(scaled_input)[0]

  fire_types = {
      0: "Vegetation Fire",
      2: "Other Static Land Source",
      3: "Offshore Fire"
  }

  result = fire_types.get(prediction, "Unknown")
  st.success(f"**Predicted Fire Type:** {result}")

!streamlit run app.py

import gradio as gr
import numpy as np
import joblib

# Load model and scaler
model = joblib.load('best_fire_detection_model.pkl')
scaler = joblib.load('scaler.pkl')

# Fire type mapping
fire_types = {
    0: "Vegetation Fire",
    2: "Other Static Land Source",
    3: "Offshore Fire"
}

# Confidence level mapping
confidence_map = {"low": 0, "nominal": 1, "high": 2}

# Prediction function
def predict_fire_type(brightness, bright_t31, frp, scan, track, confidence):
    confidence_val = confidence_map[confidence]
    input_data = np.array([[brightness, scan, track, confidence_val, bright_t31, frp]])
    scaled_input = scaler.transform(input_data)
    prediction = model.predict(scaled_input)[0]
    result = fire_types.get(prediction, "Unknown")
    return f"🔥 **Predicted Fire Type:** {result}"

# Gradio interface
interface = gr.Interface(
    fn=predict_fire_type,
    inputs=[
        gr.Number(label="Brightness", value=300.0),
        gr.Number(label="Bright_t31", value=290.0),
        gr.Number(label="Fire Radiative Power (FRP)", value=15.0),
        gr.Number(label="Scan", value=1.0),
        gr.Number(label="Track", value=1.0),
        gr.Dropdown(choices=["low", "nominal", "high"], label="Confidence level", value="nominal")
    ],
    outputs=gr.Textbox(label="Prediction"),
    title="🔥 Fire Detection App",
    description="Predict fire type based on MODIS satellite readings."
)

interface.launch()

!pip install catboost